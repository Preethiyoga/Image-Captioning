{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d14c81-b922-4402-b050-f71fd47e5c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split ## to split data set into train, test, valid\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer ## to create mapped number list for each sentence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences ## to pad the sentences in order to keep all the sentence in same length.4\n",
    "import os ## to have functions related to os directory\n",
    "\n",
    "from tensorflow.keras.preprocessing import image  ## to preprocess and collect featureset for each image using the inceptionv3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e11fa-7e41-411e-834d-b02b7390060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a442702-0cd5-41ee-9d3e-e06890743ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict = defaultdict(list) ## creating a dictionary of key: list pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b01f15-e352-40cf-9592-6ce402214c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_caption(caption):\n",
    "    caption = caption.lower() ## Converting the text letters to lower case\n",
    "    caption = caption.strip()\n",
    "    caption =  re.sub(r'[^\\w\\s]', '', caption)\n",
    "    return f\"<start>{caption} <end>\" ## including <start> and <end> tags in your captions is exactly what we need for training a sequence-to-sequence model like in image captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86ec14c-6f5c-4fb0-9676-897ba2ff262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Reading from the text file \n",
    "with open(\"captions.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip() ## To rermove leading and trailing whitespace\n",
    "        if not line:\n",
    "            continue\n",
    "        name, caption = line.split(\",\",1) ## Split only at the first encouonter of comma\n",
    "        cleanCap= clean_caption(caption)\n",
    "        captions_dict[name].append(cleanCap)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30daf849-2f62-40ff-b847-ee10d1e9f387",
   "metadata": {},
   "source": [
    "## Tasks of the Model;;\n",
    "--> to identify distinct objects in the oimage and name\n",
    "--> form a sentence by learning language from training sentence\n",
    "--> Map the sentence to the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ff147-7373-4cb0-9495-5c1e9fea00ce",
   "metadata": {},
   "source": [
    "1.) Feature Extraction from Images\n",
    "2.) Tokenising "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befddf32-88e9-4fae-ae96-cb75555c9352",
   "metadata": {},
   "source": [
    "###  Image Feature Extraction using Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39b9f94-a38e-42cc-86a4-247e1dfb65a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training data captions need to be tokenized\n",
    "imageFeatureExt = InceptionV3(include_top = False, weights = 'imagenet')\n",
    "# Add a global average pooling layer to flatten the output\n",
    "x = imageFeatureExt.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Define the new model\n",
    "model = Model(inputs=imageFeatureExt.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d738a9a4-de54-4a40-bf17-927bf9922496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59fb2f-65dc-4576-9a11-e82840ab92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Freeze the base model.. from being trained \n",
    "imageFeatureExt.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68020eac-fc71-4b6f-9a3b-54cf064f246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To extract feature set of Each Image via Inception v3 model\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size = (299, 299))  # inception v3 model expects input of size 299.299\n",
    "    img_arr = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_arr, axis=0)  ## to add one more column of batch ;; because tensorflow models expects all the input in batc format\n",
    "    img_array = preprocess_input(img_array) ## the model needs right range and distribution of pixels.. before processing\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7312ad8-4849-407b-8c0c-b9e27f6eccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replacing the image names with the feature set array that can be extracted using the InceptionV3 model\n",
    "imgFScaption = []\n",
    "\n",
    "for img, captions in captions_dict.items():\n",
    "    if img != \"image\":\n",
    "        path = os.path.join(\"./Images/\",img)\n",
    "        if os.path.exists(path):\n",
    "            print(path)\n",
    "            fs = preprocess_image(path)\n",
    "            features = model.predict(fs, verbose=0)[0] ## the output will be of batch type.. so extracting only the first feature set\n",
    "            for cap in captions:\n",
    "                imgFScaption.append((features,cap))\n",
    "\n",
    "## each line would be of image_name and caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2667aea8-d934-4db6-aee7-9e769e32b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imgFScaption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e1aaa5-217f-42b7-9a30-eafe784cf467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a5f0ab-d422-4d36-9449-7159e6cc4df5",
   "metadata": {},
   "source": [
    "Why split before tokenization?\n",
    "To avoid data leakage.\n",
    "\n",
    "You want the tokenizer to learn only from the training captions.\n",
    "\n",
    "If you fit it on all captions (train + val), the model might \"peek\" into unseen data indirectly — which defeats the purpose of validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bede0e1-f388-4510-85ef-8c9625aede85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shuffling data of caption list\n",
    "\n",
    "random.shuffle(imgFScaption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a99cb-bd39-4620-979c-ea5a77cd3a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting data \n",
    "\n",
    "train, temp = train_test_split(imgFScaption, test_size =0.2, random_state = 42)\n",
    "\n",
    "valid, test = train_test_split(temp, test_size = 0.5 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba0296-fe42-45b0-9c4f-7e0c670da0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b10c44-9615-4383-b037-6b7134b2db45",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213c259-8774-467d-bb12-85b226441c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000, oov_token = '<unk>') ## the tokens having first 5000 highest frequency would be considered and others are replaces as '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1629a6f-2204-4f08-8b23-347d6601bd72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50405f1-dcc4-4684-9492-bbb2b8d622b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(imgFScaption, train=0):\n",
    "    ## creating a list consisting only of the caption  of all the images\n",
    "    all_captions = [ caption for _, caption in imgFScaption]## collect only the cpation part from the tuple imge: caption\n",
    "    if train == 1: ## to fit the toknizer model only on the training data, we are keeping a flag \"train\" to indicate training data\n",
    "        tokenizer.fit_on_texts(all_captions) ##builds vocabulary for the mentioned captions\n",
    "    sequences = tokenizer.texts_to_sequences(all_captions) ## convert the text t sequence of numbers\n",
    "    ## padding the sequences to be of same length\n",
    "    max_length = max(len(seq) for seq in sequences)## find max length among thr sequences\n",
    "    print(max_length)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen= max_length,padding = 'post')\n",
    "    # Now need to pair up the captions with the respective images\n",
    "    FS_caption = [(fs, padseq) for (fs, _) ,padseq in zip(imgFScaption, padded_sequences)]\n",
    "    return max_length,FS_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cde150-6034-4141-acae-b0001a1ee9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c769995-9b46-4921-bc6c-cc3ce94d4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `tokenizer` is already trained\n",
    "vocab = tokenizer.word_index  # word -> index mapping\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# To see the top 20 most frequent words\n",
    "for word, idx in sorted(vocab.items(), key=lambda item: item[1])[:]:\n",
    "    print(f\"{idx}: {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace2a43c-a942-4f78-9986-b80904e4df60",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len, FS_caption = padding(train, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b562f12e-1b45-49c7-8f4a-9c5596c91d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "vz = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc9ec1-7fc1-450a-b34c-ba7e103c0fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FS_caption[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878ff1a-ad08-4770-880f-3d96a649e3a6",
   "metadata": {},
   "source": [
    "### Tokenizing and feature extraction are completed \n",
    "######\n",
    "####\n",
    "##\n",
    "### Now Build the image captioning Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1131d979-b357-496e-90d9-5a2505506504",
   "metadata": {},
   "source": [
    "           Image                             Partial Caption\n",
    "       -------------                         -----------------\n",
    "      | CNN (InceptionV3) |                 | Embedding Layer |\n",
    "       -------------                         -----------------\n",
    "              |                                      |\n",
    "     Dense Layer (feature vector)              LSTM Layer\n",
    "              |                                      |\n",
    "              |-------- Concatenate -----------------|\n",
    "                            |\n",
    "                        Dense Layer\n",
    "                            |\n",
    "                    Softmax (Vocab Size)\n",
    "                            ↓\n",
    "                Predict Next Word in Caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226dd6e7-56f3-4809-9760-30df1272743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dense layer after Feature Extraction\n",
    "image_input = Input(shape=(2048,),name = 'image_input')\n",
    "caption_input = Input(shape = (max_len,), name = 'caption_input')\n",
    "img_dense = Dense(256, activation= 'relu')(image_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b83b07-a7b1-4984-8fae-483e3b75a1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9de5509-a35d-4e2f-ab13-769fdaf1f94d",
   "metadata": {},
   "source": [
    "### Embedding Layer and LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077e7c3-e20b-45ae-bcd9-57cd3aac32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding = Embedding(input_dim=vz, output_dim=256, mask_zero=True)(caption_input)\n",
    "lstm_out = LSTM(256)(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da2053c-e3a2-407a-832b-710156780043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7dc4acf-cfcd-4845-b61c-f4247660ec08",
   "metadata": {},
   "source": [
    "## Combining these models into a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689ae16a-850a-43bb-8438-b4a2a110908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate = Concatenate()([img_dense, lstm_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c2c20-b3b8-4b19-a3f9-bf7d710048d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dense = Dense(256, activation ='relu')(concatenate)\n",
    "final_dense = Dropout(0.5)(final_dense)\n",
    "output= Dense(vz, activation ='softmax')(final_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6266e9c9-1548-4f69-9391-ddd0e5e3e684",
   "metadata": {},
   "source": [
    "### Now combining altogether and training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f447dc0-d2c8-4996-b52b-5127431e14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_inp= np.array([pair[0] for pair in FS_caption])\n",
    "cap_inp = np.array([pair[1] for pair in FS_caption])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f9149-ed64-4cd8-9636-a5402047acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs= [image_input, caption_input], outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69560fa-3725-4b39-bee3-74ed7f814f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a22bd7a-6bd8-40ab-b48e-6eb351571850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1dc8e-c677-4dc1-bc54-628dc3affa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(FS_caption, max_len, batch_size,vz):\n",
    "    while True:\n",
    "        X1, X2, y = [], [], []\n",
    "        for img_feat, caption in FS_caption:\n",
    "            for i in range(1, len(caption)):\n",
    "                in_seq = caption[:i]\n",
    "                out_word = caption[i]\n",
    "                \n",
    "                in_seq_padded = pad_sequences([in_seq], maxlen=max_len, padding='post')[0]\n",
    "                \n",
    "                X1.append(img_feat)\n",
    "                X2.append(in_seq_padded)\n",
    "                y.append(out_word)\n",
    "                \n",
    "                if len(X1) == batch_size:\n",
    "                    yield (np.array(X1), np.array(X2)), tf.keras.utils.to_categorical(y, num_classes=vz)\n",
    "                    X1, X2, y = [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627d324-89d1-47a5-92bc-373826875016",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f5acd-c8b1-4183-b3c6-6301eb56efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = (\n",
    "    (\n",
    "        tf.TensorSpec(shape=(None, 2048), dtype=tf.float16),     # image features\n",
    "        tf.TensorSpec(shape=(None, max_len), dtype=tf.int32)     # padded caption input\n",
    "    ),\n",
    "    tf.TensorSpec(shape=(None,vz ), dtype=tf.float32)    # one-hot encoded next word\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e534db-cc13-4288-b54e-c8a4f6fcbf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_generator(FS_caption,max_len, bs,vz):\n",
    "    print(\"Image shape:\", data[0][0].shape)\n",
    "    print(\"Caption input shape:\", data[0][1])\n",
    "    print(\"Label shape:\", data[1].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b596e3aa-1963-4bb3-a29f-ff2eddf257ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(FS_caption, max_len, bs,vz),\n",
    "    output_signature=output_signature\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637bdf78-ee49-4b7d-9568-0f3f06cf26c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d712e0-9e77-4f98-bd5b-5940e8ccc8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = sum(len(caption)-1 for _, caption in FS_caption) // bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a60c90-1b70-4fca-a17e-82c2c19a3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset, epochs=20, steps_per_epoch=steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ff9bc-3a63-4170-b6e9-02f837d48363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73a3c1-c1d0-4dd4-95aa-6e81e1c6b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2002c5-20bb-48cb-b8fa-bf359e51f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"img_captioning_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936880c6-af96-40ae-b130-4598a87e71db",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec15d14-b328-4e3d-993e-fc2bdb7852c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_valid, valid = padding(valid,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d791c8c1-1c2e-4197-81b0-a5220c2284c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_signature = (\n",
    "    (\n",
    "        tf.TensorSpec(shape=(None, 2048), dtype=tf.float16),     # image features\n",
    "        tf.TensorSpec(shape=(None, max_len), dtype=tf.int32)     # padded caption input\n",
    "    ),\n",
    "    tf.TensorSpec(shape=(None,vz ), dtype=tf.float32)    # one-hot encoded next word\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f64cef-8bd1-421f-923e-a3c9c4a535fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_generator(valid,max_len, bs):\n",
    "    print(\"Image shape:\", data[0][0].shape)\n",
    "    print(\"Caption input shape:\", data[0][1])\n",
    "    print(\"Label shape:\", data[1].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9e7d4-f124-4e9d-b271-3edec1e870cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(valid, max_len, bs),\n",
    "    output_signature=valid_signature\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc75b8-e6f9-41db-b73d-4da93de2ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = sum(len(caption)-1 for _, caption in valid) // bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa5f7d-3c6f-4543-a9a7-cbe12815bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(dataset,steps = steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4665695c-1df8-4be9-8d63-c71b32dd4c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model1 = Model(inputs=imageFeatureExt.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e69178d-3459-488b-a0cb-9b2f8082cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, tokenizer, photo, max_length):\n",
    "    in_text = '<start>'\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = tokenizer.index_word.get(yhat, None)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == '<end>':\n",
    "            break\n",
    "    return in_text.replace('<start>', '').replace('end', '').strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e401e-baf3-4c74-ac9a-359a2a889787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def display_image_and_caption(image_path, model, tokenizer, max_length, feature_extractor):\n",
    "    # Extract features from image (if not already done)\n",
    "    image = Image.open(image_path)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    photo = feature_extractor(image_path)  # replace with your feature extractor\n",
    "    photo = model1.predict(photo, verbose=0)[0]\n",
    "    photo = np.expand_dims(photo, axis=0)  # add batch dimension\n",
    "\n",
    "    # Generate caption\n",
    "    caption = generate_caption(model, tokenizer, photo, max_length)\n",
    "    \n",
    "    plt.title(caption)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d44959-61fc-4a8d-a8e4-aabbfb4d650e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_image_and_caption(\"test.jpg\", model, tokenizer, max_len, preprocess_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073ce3e-4e1d-4d56-b83b-6e27e95ecaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff109edd-8300-41b5-98f6-672374e72488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
